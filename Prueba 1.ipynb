{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from Library1 import Data_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
      "from pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler as PySparkMinMaxScaler\n",
      "from pyspark.sql import DataFrame as PySparkDataFrame\n",
      "\n",
      "class Data_utils:\n",
      "    def __init__(self):\n",
      "        pass\n",
      "\n",
      "    def one_hot_pandas(self, dataframe, column_name):\n",
      "        df_column = dataframe[column_name].values\n",
      "        onehot_encoder = OneHotEncoder(sparse_output=False)\n",
      "        onehot_list = onehot_encoder.fit_transform(df_column.reshape(-1, 1)).tolist()\n",
      "        new_dataframe = pd.DataFrame(onehot_list, columns=onehot_encoder.get_feature_names_out([column_name]))\n",
      "        dataframe = pd.concat([dataframe.drop(columns=[column_name]), new_dataframe], axis=1)\n",
      "        return dataframe\n",
      "\n",
      "    def ordinal_encoder(self, dataframe, column_name):\n",
      "        list_category = dataframe[column_name].unique()\n",
      "        ordinal_encoder = OrdinalEncoder(categories=[list_category])\n",
      "        ordinal_list = ordinal_encoder.fit_transform(dataframe[column_name].values.reshape(-1, 1)).tolist()\n",
      "        for index in range(len(ordinal_list)):\n",
      "            ordinal_list[index] = ordinal_list[index][0]\n",
      "        return ordinal_list\n",
      "\n",
      "    def min_max_scaler_pandas(self, dataframe, columns_name):\n",
      "        scaler = MinMaxScaler()\n",
      "        for column_name in columns_name:\n",
      "            data_list = scaler.fit_transform(dataframe[column_name].values.reshape(-1, 1)).tolist()\n",
      "            for index in range(len(data_list)):\n",
      "                data_list[index] = data_list[index][0]\n",
      "            dataframe[column_name] = data_list\n",
      "        return dataframe\n",
      "\n",
      "    def ordinal_encoder_pyspark(self, dataframe, list_column_names):\n",
      "        for column in list_column_names:\n",
      "            indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\")\n",
      "            dataframe = indexer.fit(dataframe).transform(dataframe)\n",
      "        return dataframe\n",
      "\n",
      "    def min_max_scaler_pyspark(self, dataframe, columns_name):\n",
      "        assembler = VectorAssembler(inputCols=columns_name, outputCol=\"num_features\")\n",
      "        df_assembler = assembler.transform(dataframe)\n",
      "        scaler = PySparkMinMaxScaler(inputCol=\"num_features\", outputCol=\"scaled_features\")\n",
      "        scaler_model = scaler.fit(df_assembler)\n",
      "        df_scaled = scaler_model.transform(df_assembler)\n",
      "        return df_scaled\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leer y mostrar el contenido de Library1.py\n",
    "with open('Library1.py', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "boston_df_spark = spark.read.csv('boston.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Convertir a un DataFrame de pandas\n",
    "boston_df_pandas = boston_df_spark.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del DataFrame de pandas:\n",
      "Index(['_c0', 'CRIM', 'RM', 'DIS', 'PTRATIO', 'LSTAT', 'GFK', 'MEDV'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Verificar las columnas del DataFrame de pandas\n",
    "print(\"Columnas del DataFrame de pandas:\")\n",
    "print(boston_df_pandas.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del DataFrame de PySpark:\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- CRIM: double (nullable = true)\n",
      " |-- RM: double (nullable = true)\n",
      " |-- DIS: double (nullable = true)\n",
      " |-- PTRATIO: double (nullable = true)\n",
      " |-- LSTAT: double (nullable = true)\n",
      " |-- GFK: string (nullable = true)\n",
      " |-- MEDV: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar las columnas del DataFrame de PySpark\n",
    "print(\"Columnas del DataFrame de PySpark:\")\n",
    "boston_df_spark.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(_c0=0, CRIM=0.00632, RM=6.575, DIS=4.09, PTRATIO=15.3, LSTAT=4.98, GFK='YES', MEDV=24.0)\n"
     ]
    }
   ],
   "source": [
    "print(boston_df_spark.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   _c0     CRIM     RM     DIS  PTRATIO  LSTAT  GFK  MEDV\n",
      "0    0  0.00632  6.575  4.0900     15.3   4.98  YES  24.0\n",
      "1    1  0.02731  6.421  4.9671     17.8   9.14  YES  21.6\n",
      "2    2  0.02729  7.185  4.9671     17.8   4.03  YES  34.7\n",
      "3    3  0.03237  6.998  6.0622     18.7   2.94   NO  33.4\n",
      "4    4  0.06905  7.147  6.0622     18.7   5.33   NO  36.2\n"
     ]
    }
   ],
   "source": [
    "print(boston_df_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils = Data_utils()\n",
    "columns_name = ['CRIM', 'RM', 'DIS', 'PTRATIO', 'LSTAT', 'MEDV']\n",
    "categorical_columns_pandas = ['GFK']\n",
    "categorical_columns_pyspark = ['GFK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df_pandas_one_hot = data_utils.one_hot_pandas(boston_df_pandas, 'GFK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista ordinal: [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "ordinal_list = data_utils.ordinal_encoder(boston_df_pandas, 'GFK')\n",
    "print(\"Lista ordinal:\", ordinal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de pandas transformado:\n",
      "   _c0      CRIM        RM       DIS   PTRATIO     LSTAT  GFK      MEDV\n",
      "0    0  0.000000  0.577505  0.269203  0.287234  0.089680  YES  0.422222\n",
      "1    1  0.000236  0.547998  0.348962  0.553191  0.204470  YES  0.368889\n",
      "2    2  0.000236  0.694386  0.348962  0.553191  0.063466  YES  0.660000\n",
      "3    3  0.000293  0.658555  0.448545  0.648936  0.033389   NO  0.631111\n",
      "4    4  0.000705  0.687105  0.448545  0.648936  0.099338   NO  0.693333\n"
     ]
    }
   ],
   "source": [
    "boston_df_pandas_scaled = data_utils.min_max_scaler_pandas(boston_df_pandas, columns_name)\n",
    "print(\"DataFrame de pandas transformado:\")\n",
    "print(boston_df_pandas_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de PySpark con codificación ordinal:\n",
      "+---+-------+-----+------+-------+-----+---+----+---------+\n",
      "|_c0|   CRIM|   RM|   DIS|PTRATIO|LSTAT|GFK|MEDV|GFK_index|\n",
      "+---+-------+-----+------+-------+-----+---+----+---------+\n",
      "|  0|0.00632|6.575|  4.09|   15.3| 4.98|YES|24.0|      1.0|\n",
      "|  1|0.02731|6.421|4.9671|   17.8| 9.14|YES|21.6|      1.0|\n",
      "|  2|0.02729|7.185|4.9671|   17.8| 4.03|YES|34.7|      1.0|\n",
      "|  3|0.03237|6.998|6.0622|   18.7| 2.94| NO|33.4|      0.0|\n",
      "|  4|0.06905|7.147|6.0622|   18.7| 5.33| NO|36.2|      0.0|\n",
      "|  5|0.02985| 6.43|6.0622|   18.7| 5.21| NO|28.7|      0.0|\n",
      "|  6|0.08829|6.012|5.5605|   15.2|12.43|YES|22.9|      1.0|\n",
      "|  7|0.14455|6.172|5.9505|   15.2|19.15|YES|27.1|      1.0|\n",
      "|  8|0.21124|5.631|6.0821|   15.2|29.93| NO|16.5|      0.0|\n",
      "|  9|0.17004|6.004|6.5921|   15.2| 17.1| NO|18.9|      0.0|\n",
      "| 10|0.22489|6.377|6.3467|   15.2|20.45| NO|15.0|      0.0|\n",
      "| 11|0.11747|6.009|6.2267|   15.2|13.27| NO|18.9|      0.0|\n",
      "| 12|0.09378|5.889|5.4509|   15.2|15.71|YES|21.7|      1.0|\n",
      "| 13|0.62976|5.949|4.7075|   21.0| 8.26| NO|20.4|      0.0|\n",
      "| 14|0.63796|6.096|4.4619|   21.0|10.26| NO|18.2|      0.0|\n",
      "| 15|0.62739|5.834|4.4986|   21.0| 8.47| NO|19.9|      0.0|\n",
      "| 16|1.05393|5.935|4.4986|   21.0| 6.58| NO|23.1|      0.0|\n",
      "| 17| 0.7842| 5.99|4.2579|   21.0|14.67| NO|17.5|      0.0|\n",
      "| 18|0.80271|5.456|3.7965|   21.0|11.69| NO|20.2|      0.0|\n",
      "| 19| 0.7258|5.727|3.7965|   21.0|11.28| NO|18.2|      0.0|\n",
      "+---+-------+-----+------+-------+-----+---+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boston_df_spark_ordinal = data_utils.ordinal_encoder_pyspark(boston_df_spark, categorical_columns_pyspark)\n",
    "print(\"DataFrame de PySpark con codificación ordinal:\")\n",
    "boston_df_spark_ordinal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de PySpark transformado:\n",
      "+---+-------+-----+------+-------+-----+---+----+--------------------+--------------------+\n",
      "|_c0|   CRIM|   RM|   DIS|PTRATIO|LSTAT|GFK|MEDV|        num_features|     scaled_features|\n",
      "+---+-------+-----+------+-------+-----+---+----+--------------------+--------------------+\n",
      "|  0|0.00632|6.575|  4.09|   15.3| 4.98|YES|24.0|[0.00632,6.575,4....|[0.0,0.5775052692...|\n",
      "|  1|0.02731|6.421|4.9671|   17.8| 9.14|YES|21.6|[0.02731,6.421,4....|[2.35922539178427...|\n",
      "|  2|0.02729|7.185|4.9671|   17.8| 4.03|YES|34.7|[0.02729,7.185,4....|[2.35697744000553...|\n",
      "|  3|0.03237|6.998|6.0622|   18.7| 2.94| NO|33.4|[0.03237,6.998,6....|[2.92795719180468...|\n",
      "|  4|0.06905|7.147|6.0622|   18.7| 5.33| NO|36.2|[0.06905,7.147,6....|[7.05070075400798...|\n",
      "|  5|0.02985| 6.43|6.0622|   18.7| 5.21| NO|28.7|[0.02985,6.43,6.0...|[2.64471526768385...|\n",
      "|  6|0.08829|6.012|5.5605|   15.2|12.43|YES|22.9|[0.08829,6.012,5....|[9.21323036515279...|\n",
      "|  7|0.14455|6.172|5.9505|   15.2|19.15|YES|27.1|[0.14455,6.172,5....|[0.00155367187187...|\n",
      "|  8|0.21124|5.631|6.0821|   15.2|29.93| NO|16.5|[0.21124,5.631,6....|[0.00230325139249...|\n",
      "|  9|0.17004|6.004|6.5921|   15.2| 17.1| NO|18.9|[0.17004,6.004,6....|[0.00184017332607...|\n",
      "| 10|0.22489|6.377|6.3467|   15.2|20.45| NO|15.0|[0.22489,6.377,6....|[0.00245667410139...|\n",
      "| 11|0.11747|6.009|6.2267|   15.2|13.27| NO|18.9|[0.11747,6.009,6....|[0.00124929920103...|\n",
      "| 12|0.09378|5.889|5.4509|   15.2|15.71|YES|21.7|[0.09378,5.889,5....|[9.83029312841604...|\n",
      "| 13|0.62976|5.949|4.7075|   21.0| 8.26| NO|20.4|[0.62976,5.949,4....|[0.00700731528467...|\n",
      "| 14|0.63796|6.096|4.4619|   21.0|10.26| NO|18.2|[0.63796,6.096,4....|[0.00709948130760...|\n",
      "| 15|0.62739|5.834|4.4986|   21.0| 8.47| NO|19.9|[0.62739,5.834,4....|[0.00698067705610...|\n",
      "| 16|1.05393|5.935|4.4986|   21.0| 6.58| NO|23.1|[1.05393,5.935,4....|[0.01177488381461...|\n",
      "| 17| 0.7842| 5.99|4.2579|   21.0|14.67| NO|17.5|[0.7842,5.99,4.25...|[0.00874318364821...|\n",
      "| 18|0.80271|5.456|3.7965|   21.0|11.69| NO|20.2|[0.80271,5.456,3....|[0.00895123158534...|\n",
      "| 19| 0.7258|5.727|3.7965|   21.0|11.28| NO|18.2|[0.7258,5.727,3.7...|[0.00808678172882...|\n",
      "+---+-------+-----+------+-------+-----+---+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boston_df_spark_scaled = data_utils.min_max_scaler_pyspark(boston_df_spark, columns_name)\n",
    "print(\"DataFrame de PySpark transformado:\")\n",
    "boston_df_spark_scaled.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
